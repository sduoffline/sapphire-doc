# Segment Anything(SAM)

## Introduction

SAM 是 Meta 提出的一种通用的、（准）多模态的 Zero-Shot 分割模型，极大地促进了计算机视觉基础模型的发展。

SAM是一个提示型模型，其在1100万张图像上训练了超过10亿个 Mask，实现了强大的零样本泛化。许多研究人员认为「这是 CV 的 GPT-3 时刻，因为 SAM 已经学会了物体是什么的一般概念，甚至是未知的物体、不熟悉的场景（如水下、细胞显微镜）和模糊的情况」，并展示了作为 CV 基本模型的巨大潜力。

SAM 的输入与输出结构比较简单，其输入是一张图像和 Prompt，输出是图像的分割结果，与传统的分割网络相比并无太大区别。SAM 的核心在于输入的 Prompt，Prompt 是一种提示信息，用于指导模型进行分割，这也是 SAM 能够实现通用分割的关键。SAM 支持四种 Prompt 类型：`mask`、`points`、`box`和`text`，分别对应分割掩码、关键点、边界框和文本信息。不过，文本提示的效果并不理想，因此更多的使用 CLIP 配合 SAM 实现文本引导的分割任务。

在模型结构上，与现在大量 Decoder Only（以GPT为代表）的模型不同，SAM 仍然是一款建立在传统 Encoder-Decoder 结构、卷积神经网络的基础上的模型。SAM 包括三个主要结构：`Image Encoder`、`Prompt Encoder`和`Mask Decoder`，其中`Prompt Encoder`是 SAM 的核心，它将图像和提示信息进行编码，然后输入到`Mask Decoder`中，生成分割结果。

在实际应用中，SAM 通常可以视作 `Image Embedding` 和 `Segment` 两个任务阶段。在 `Image Embedding` 过程中，SAM 需要使用 `ViT` 对图像进行编码，这个过程是相对耗时的。而在 `Segment` 过程中，SAM 只需要对 `Prompt` 和 `Image Embedding` 进行编码，然后输入到 `Mask Decoder` 中，生成分割结果。因此，SAM 的推理速度相对较快。

## Background

### Zero-Shot Segmentation

视觉任务大致分为分类、检测和分割三类，其中分割是最为复杂的任务。传统的分割任务通常需要大量的标注数据，而零样本分割则是一种**无需标注数据**的分割任务。

零样本分割的关键在于模型的泛化能力，即**模型能够在未见过的数据上进行分割**。这意味着，对于 Zero-Shot Segmentation 任务，模型需要具备以下能力：

1. **通用性**：模型能够对任意物体进行分割，而不仅仅是训练集中的物体。
2. **泛化性**：模型能够在未见过的数据上进行分割，而不仅仅是训练集中的数据。

为了训练零样本分割模型，往往需要构建一个十分巨大、包含各种物体的数据集。然而，这种数据集的构建是十分困难的，因此 Zero-Shot Segmentation 一直是计算机视觉领域的一个热门研究方向。

### Pre-training

随着神经网络结构的不断发展，预训练技术逐渐成为了计算机视觉领域的主流。预训练技术的核心在于**利用大规模数据集进行训练**，然后将训练好的模型迁移到其他任务上进行微调。

在传统图像处理任务中，ResNet、VGG 等网络都是典型的预训练模型，或者称为 `Backbone`。这些预训练模型通常会在一个大的数据集上进行训练，具有良好的特征提取能力。通过使用 `Backbone` 对图像进行特征提取，并在此基础上添加新的模型结构进行训练，往往能获得更高的性能、缩短训练时间。

预训练解决的核心问题在于神经网络规模不断增大时，数据集和计算资源的需求也在不断增加。通过引入 Pre-training，大量的下游任务可以充分利用 `Backbone` 强大的特征提取能力再进行训练，有效降低了下游任务的模型规模和训练时间。

### ONNX Runtime

神经网络的运行和部署是一个十分重要的问题。对于科研人员而言，`PyTorch`和`TensorFlow`等框架提供了便捷的训练和调试环境，但是这些框架往往不适合直接部署到生产环境中。

首先，这些框架往往对运行环境和硬件设备有一定的要求，而生产环境往往是多样化的，这就需要对模型进行一定的转换。其次，这些框架的运行效率往往不高，对于一些对速度要求较高的场景，这就需要对模型进行一定的优化。

在模型层面上，投入生产和部署时常常会进行量化压缩操作，以减少模型的体积和加速模型的推理速度。这是由于多数神经网络在训练阶段使用`fp32`或`fp16`的浮点数进行计算，而在推理阶段往往可以使用更低精度的整数（`int8`）进行计算，以减少计算量，并且对 CPU 和 GPU 的支持更好。

运行层面上主要有两种思路，分别是通用运行时和编译运行：

- 通用运行时：通用运行时是一种支持多种硬件设备的运行时，如`ONNX Runtime`。`ONNX Runtime` 是一个开源的深度学习推理引擎，支持 ONNX 格式的模型，并且支持多种硬件设备，如 CPU、GPU、FPGA 等。
- 编译运行：编译运行是一种将模型编译成特定硬件设备的指令集，以提高模型的运行效率。这种方式往往需要对模型进行一定的优化，如量化、剪枝等。主要框架有`ncnn`、`MNN`等。

`ONNX` 现在由微软维护，他的实现思路是通过记录模型的计算图，得到模型的计算过程，生成一个计算图的执行计划（类似`Java`编译后的字节码）；在设备上，通过 `ONNX Runtime` 加载模型，执行计算图的执行计划（类似`JVM`），从而实现模型的推理。

## Method

### Model Structure

[Model Structure](https://image.thuray.xyz/2024/03/bcf66e256e06eaa45413b4dba28e99ae.png)

SAM 的结构可以说相当简单，包括三部分：

- `Image Encoder`：用于对图像进行编码，提取图像的特征。SAM 选择使用 ViT 作为图像特征提取的模型，默认为 `Vit-h`。`ViT` 是一种基于 Transformer 的图像分类模型，其将图像划分为一系列的 Patch，然后输入到 Transformer 中进行特征提取，相比 Resnet 等传统模型，ViT 具有更好的特征提取能力。
- `Prompt Encoder`：用于对 Prompt 进行编码，提取 Prompt 的特征。
- `Mask Decoder`：用于将 Image Encoder 和 Prompt Encoder 的特征进行融合，生成分割结果。

### Training Strategy

## Results

## References
